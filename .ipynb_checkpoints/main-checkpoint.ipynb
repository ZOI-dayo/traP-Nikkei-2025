{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-07T17:27:55.473199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# データ読み込み\n",
    "DATA_DIR = './tracon01/'\n",
    "train = pl.read_csv(DATA_DIR + 'train.csv')\n",
    "test = pl.read_csv(DATA_DIR + 'test.csv')\n",
    "\n",
    "print(train)\n",
    "print(test)\n",
    "\n",
    "# レポジトリ情報を読み取り\n",
    "repo = pl.read_csv(DATA_DIR + 'repo.csv')\n",
    "\n",
    "# repo_idをもとにレポジトリ情報を結合\n",
    "train_merged = train.join(repo, on='repo_id', how='left')\n",
    "test_merged = test.join(repo, on='repo_id', how='left')\n",
    "\n",
    "\n",
    "def add_col(df, name, add):\n",
    "    return df.with_columns(add.alias(name))\n",
    "\n",
    "\n",
    "def get_readme_size(s):\n",
    "    l = eval(s)\n",
    "    for e in l:\n",
    "        if e[\"name\"] == \"README.md\":\n",
    "            return e[\"size\"]\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_file_size(s):\n",
    "    l = eval(s)\n",
    "    res = 0\n",
    "    for e in l:\n",
    "        res += e[\"size\"]\n",
    "    return res\n",
    "\n",
    "\n",
    "train_merged = add_col(train_merged, \"readme_size\", train_merged[\"files\"].map_elements(get_readme_size))\n",
    "test_merged = add_col(test_merged, \"readme_size\", test_merged[\"files\"].map_elements(get_readme_size))\n",
    "\n",
    "train_merged = add_col(train_merged, \"file_size\", train_merged[\"files\"].map_elements(get_file_size))\n",
    "test_merged = add_col(test_merged, \"file_size\", test_merged[\"files\"].map_elements(get_file_size))\n",
    "\n",
    "readme_size_cnt = {}\n",
    "\n",
    "for row in train_merged.iter_rows(named=True):\n",
    "    readme_size_cnt[row[\"readme_size\"]] = readme_size_cnt.get(row[\"readme_size\"], 0) + 1\n",
    "for row in test_merged.iter_rows(named=True):\n",
    "    readme_size_cnt[row[\"readme_size\"]] = readme_size_cnt.get(row[\"readme_size\"], 0) + 1\n",
    "\n",
    "\n",
    "def get_readme_count(readme_size):\n",
    "    return readme_size_cnt[readme_size]\n",
    "\n",
    "\n",
    "train_merged = add_col(train_merged, \"readme_size_cnt\", train_merged[\"readme_size\"].map_elements(get_readme_count))\n",
    "test_merged = add_col(test_merged, \"readme_size_cnt\", test_merged[\"readme_size\"].map_elements(get_readme_count))\n",
    "\n",
    "print(train_merged)\n",
    "\n",
    "# repo urlを追加\n",
    "train_merged = add_col(train_merged, \"repo_url\", train_merged[\"owner\"] + \"/\" + train_merged[\"repo\"])\n",
    "test_merged = add_col(test_merged, \"repo_url\", test_merged[\"owner\"] + \"/\" + test_merged[\"repo\"])\n",
    "# train_merged = train_merged.with_columns((train_merged[\"owner\"] + \"/\" + train_merged[\"repo\"]).alias(\"repo_url\"))\n",
    "# test_merged = test_merged.with_columns((test_merged[\"owner\"] + \"/\" + test_merged[\"repo\"]).alias(\"repo_url\"))\n"
   ],
   "id": "555e4f335e039fcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3_683, 4)\n",
      "┌─────────────────────────────────┬──────────────┬────────────────────────────────┬────────┐\n",
      "│ repo_id                         ┆ owner        ┆ repo                           ┆ active │\n",
      "│ ---                             ┆ ---          ┆ ---                            ┆ ---    │\n",
      "│ str                             ┆ str          ┆ str                            ┆ bool   │\n",
      "╞═════════════════════════════════╪══════════════╪════════════════════════════════╪════════╡\n",
      "│ 543a877d-cb0c-4705-92f5-acaf04… ┆ dana-i2cat   ┆ opennaas                       ┆ false  │\n",
      "│ 2c471519-698f-439f-a498-fc57d1… ┆ salesagility ┆ SuiteCRM                       ┆ true   │\n",
      "│ 9535bc28-6bca-4843-be86-b634c2… ┆ fake-name    ┆ ReadableWebProxy               ┆ false  │\n",
      "│ c9417209-8ae3-4352-b41e-838e19… ┆ AOSParadox   ┆ android_kernel_oneplus_msm8974 ┆ false  │\n",
      "│ 0292a511-920f-483e-aeff-bf743c… ┆ ipti         ┆ br.tag                         ┆ true   │\n",
      "│ …                               ┆ …            ┆ …                              ┆ …      │\n",
      "│ 8f53ae54-82a8-4674-994a-011733… ┆ qudou        ┆ xmlplus                        ┆ false  │\n",
      "│ 408a5695-2023-4131-a7a8-d90422… ┆ midgen       ┆ cashgenUE                      ┆ false  │\n",
      "│ 187c0179-23d2-49de-bb29-867220… ┆ FCP-INDI     ┆ C-PAC                          ┆ true   │\n",
      "│ f22e99d7-7939-4a7f-8338-54ef89… ┆ estafette    ┆ estafette-ci-builder           ┆ false  │\n",
      "│ c7765d85-0b95-4806-9554-9665d4… ┆ xmendez      ┆ wfuzz                          ┆ false  │\n",
      "└─────────────────────────────────┴──────────────┴────────────────────────────────┴────────┘\n",
      "shape: (1_815, 3)\n",
      "┌─────────────────────────────────┬────────────────────┬────────────────────────────────┐\n",
      "│ repo_id                         ┆ owner              ┆ repo                           │\n",
      "│ ---                             ┆ ---                ┆ ---                            │\n",
      "│ str                             ┆ str                ┆ str                            │\n",
      "╞═════════════════════════════════╪════════════════════╪════════════════════════════════╡\n",
      "│ aae6372a-c92c-477c-b28f-d5979d… ┆ gurugio            ┆ lowlevelprogramming-university │\n",
      "│ 424e6ca9-cd93-4ddc-a4ff-c161be… ┆ pgRouting          ┆ pgrouting                      │\n",
      "│ bbfad893-9643-4b5c-9ac6-5eab31… ┆ tensorflow         ┆ tensorflow                     │\n",
      "│ 494417c4-ba3f-4c9e-878e-aca1b2… ┆ sztupy             ┆ samsung-kernel-herring         │\n",
      "│ e92905aa-bdd2-4cd9-b67b-1192d3… ┆ eliotsykes         ┆ real-world-rails               │\n",
      "│ …                               ┆ …                  ┆ …                              │\n",
      "│ 2b671097-3b9b-4d61-bf48-44d0db… ┆ Beaconstac         ┆ Android-SDK                    │\n",
      "│ 0e305ada-9e27-4d9b-9bbc-1f46db… ┆ NahomAgidew        ┆ eCommerce                      │\n",
      "│ a00999a4-a1bb-4df0-8066-361de6… ┆ iolevel            ┆ peachpie-samples               │\n",
      "│ 6791bcd6-e573-4c77-b6c0-e9bef4… ┆ fracpete           ┆ multisearch-weka-package       │\n",
      "│ 59bd1314-b5f4-4d01-846b-28c8c5… ┆ battle-for-wesnoth ┆ svn                            │\n",
      "└─────────────────────────────────┴────────────────────┴────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# コミット情報を読み取り\n",
    "print(\"コミット情報を読み取っています...\")\n",
    "commits = pl.read_csv(DATA_DIR + 'commits_sampled_10.csv')\n",
    "\n",
    "print(\"コミット情報を集計します...\")\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "repo_commit_cnt = {}\n",
    "repo_latest_commit_date = {}\n",
    "repo_commit_members = {}\n",
    "repo_commit_message_sum = {}\n",
    "repo_recent_commit_cnt = {}\n",
    "\n",
    "for row in commits.iter_rows(named=True):\n",
    "    # print(f\"row.repo_names = {row.repo_names}\")\n",
    "    # 日付として Author Date を利用\n",
    "    if row[\"author_date\"] is None:\n",
    "        continue\n",
    "    date = dt.strptime(row[\"author_date\"], '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "    for repo_name in eval(row[\"repo_names\"]):\n",
    "        # print(f\"repo_name = {repo_name}\")\n",
    "        repo_commit_cnt[repo_name] = repo_commit_cnt.get(repo_name, 0) + 1\n",
    "        repo_latest_commit_date[repo_name] = max(repo_latest_commit_date.get(repo_name, 0), date)\n",
    "        if not repo_name in repo_commit_members:\n",
    "            repo_commit_members[repo_name] = {}\n",
    "        repo_commit_members[repo_name][row[\"author_name\"]] = repo_commit_members[repo_name].get(row[\"author_name\"],\n",
    "                                                                                                0) + 1\n",
    "        if row[\"message\"] is not None:\n",
    "            repo_commit_message_sum[repo_name] = repo_commit_message_sum.get(repo_name, 0) + len(row[\"message\"])\n",
    "        if date > 1640995200 - 31 * 24 * 60 * 60:  # 2022/1/1 = 1640995200\n",
    "            repo_recent_commit_cnt[repo_name] = repo_recent_commit_cnt.get(repo_name, 0) + 1\n",
    "\n",
    "repo_commit_cnt_df = pl.DataFrame({\"repo_url\": repo_commit_cnt.keys(), \"n_commits\": repo_commit_cnt.values()})\n",
    "repo_latest_commit_date_df = pl.DataFrame(\n",
    "    {\"repo_url\": repo_latest_commit_date.keys(), \"last_commit_date\": repo_latest_commit_date.values()})\n",
    "repo_commit_members_df = pl.DataFrame(\n",
    "    {\"repo_url\": repo_commit_members.keys(), \"n_commit_members\": [len(e) for e in repo_commit_members.values()]}\n",
    ")\n",
    "repo_commit_message_sum_df = pl.DataFrame(\n",
    "    {\"repo_url\": repo_commit_message_sum.keys(), \"len_commit_messages\": repo_commit_message_sum.values()}\n",
    ")\n",
    "repo_recent_commit_cnt_df = pl.DataFrame(\n",
    "    {\"repo_url\": repo_recent_commit_cnt.keys(), \"n_recent_commits\": repo_recent_commit_cnt.values()}\n",
    ")\n",
    "# print(repo_commit_cnt_df)\n",
    "\n",
    "print(\"コミット情報を結合します...\")\n",
    "\n",
    "print(train_merged)\n",
    "print(repo_commit_cnt_df)\n",
    "train_merged = train_merged.join(repo_commit_cnt_df, on='repo_url', how='left')\n",
    "test_merged = test_merged.join(repo_commit_cnt_df, on='repo_url', how='left')\n",
    "\n",
    "train_merged = train_merged.join(repo_latest_commit_date_df, on='repo_url', how='left')\n",
    "test_merged = test_merged.join(repo_latest_commit_date_df, on='repo_url', how='left')\n",
    "\n",
    "train_merged = train_merged.join(repo_commit_members_df, on='repo_url', how='left')\n",
    "test_merged = test_merged.join(repo_commit_members_df, on='repo_url', how='left')\n",
    "\n",
    "train_merged = train_merged.join(repo_commit_message_sum_df, on='repo_url', how='left')\n",
    "test_merged = test_merged.join(repo_commit_message_sum_df, on='repo_url', how='left')\n",
    "\n",
    "train_merged = train_merged.join(repo_recent_commit_cnt_df, on='repo_url', how='left')\n",
    "test_merged = test_merged.join(repo_recent_commit_cnt_df, on='repo_url', how='left')\n",
    "\n",
    "print(\"コミット情報を読み取れました\")\n"
   ],
   "id": "4e89a569749b96a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"issue情報を読み取っています...\")\n",
    "issues = pl.read_csv(DATA_DIR + 'issues.csv')\n",
    "print(issues)\n",
    "\n",
    "print(\"issue情報を加工しています...\")\n",
    "\n",
    "issue_count_map = {}\n",
    "issue_open_count_map = {}\n",
    "repo_latest_closed_issue_map = {}\n",
    "issue_message_len_map = {}\n",
    "for row in issues.iter_rows(named=True):\n",
    "    issue_count_map[row[\"repo_id\"]] = issue_count_map.get(row[\"repo_id\"], 0) + 1\n",
    "    if row[\"state\"] == 'open':\n",
    "        issue_open_count_map[row[\"repo_id\"]] = issue_open_count_map.get(row[\"repo_id\"], 0) + 1\n",
    "    else:\n",
    "        if row[\"closed_at\"] is not None:\n",
    "            date = dt.fromisoformat(row[\"closed_at\"]).timestamp()\n",
    "            repo_latest_closed_issue_map[row[\"repo_id\"]] = max(repo_latest_closed_issue_map.get(row[\"repo_id\"], 0),\n",
    "                                                               date)\n",
    "    if row[\"body\"] is not None:\n",
    "        issue_message_len_map[row[\"repo_id\"]] = issue_message_len_map.get(row[\"repo_id\"], 0) + len(row[\"body\"])\n",
    "issue_count_df = pl.DataFrame({\"repo_id\": issue_count_map.keys(), \"n_issues\": issue_count_map.values()})\n",
    "issue_open_count_df = pl.DataFrame(\n",
    "    {\"repo_id\": issue_open_count_map.keys(), \"n_open_issues\": issue_open_count_map.values()})\n",
    "repo_latest_closed_issue_df = pl.DataFrame(\n",
    "    {\"repo_id\": repo_latest_closed_issue_map.keys(), \"latest_closed_issue\": repo_latest_closed_issue_map.values()}\n",
    ")\n",
    "issue_message_len_df = pl.DataFrame(\n",
    "    {\"repo_id\": issue_message_len_map.keys(), \"issue_message_len\": issue_message_len_map.values()}\n",
    ")\n",
    "\n",
    "print(\"issue情報を結合しています...\")\n",
    "\n",
    "train_merged = train_merged.join(issue_count_df, on='repo_id', how='left')\n",
    "test_merged = test_merged.join(issue_count_df, on='repo_id', how='left')\n",
    "\n",
    "train_merged = train_merged.join(issue_open_count_df, on='repo_id', how='left')\n",
    "test_merged = test_merged.join(issue_open_count_df, on='repo_id', how='left')\n",
    "\n",
    "train_merged = train_merged.join(repo_latest_closed_issue_df, on='repo_id', how='left')\n",
    "test_merged = test_merged.join(repo_latest_closed_issue_df, on='repo_id', how='left')\n",
    "\n",
    "train_merged = add_col(train_merged, \"issue_open_ratio\", train_merged[\"n_open_issues\"] / train_merged[\"n_issues\"])\n",
    "test_merged = add_col(test_merged, \"issue_open_ratio\", test_merged[\"n_open_issues\"] / test_merged[\"n_issues\"])\n",
    "\n",
    "train_merged = train_merged.join(issue_message_len_df, on='repo_id', how='left')\n",
    "test_merged = test_merged.join(issue_message_len_df, on='repo_id', how='left')\n",
    "\n",
    "train_merged = add_col(train_merged, \"ave_issue_body_len\", train_merged[\"issue_message_len\"] / train_merged[\"n_issues\"]).fill_null(0).fill_nan(0)\n",
    "test_merged = add_col(test_merged, \"ave_issue_body_len\", test_merged[\"issue_message_len\"] / test_merged[\"n_issues\"]).fill_null(0).fill_nan(0)\n",
    "\n",
    "print(\"issue情報の取り込みが完了しました\")\n"
   ],
   "id": "650772c5e3c6ddea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print(\"PR情報を読み取っています...\")\n",
    "issues = pl.read_csv(DATA_DIR + 'pulls.csv')\n",
    "\n",
    "print(\"PR情報を加工しています...\")\n",
    "\n",
    "pull_count_map = {}\n",
    "pull_open_count_map = {}\n",
    "for row in issues.iter_rows(named=True):\n",
    "    pull_count_map[row[\"repo_id\"]] = pull_count_map.get(row[\"repo_id\"], 0) + 1\n",
    "    if row[\"state\"] == 'open':\n",
    "        pull_open_count_map[row[\"repo_id\"]] = pull_open_count_map.get(row[\"repo_id\"], 0) + 1\n",
    "pull_count_df = pl.DataFrame({\"repo_id\": pull_count_map.keys(), \"n_pulls\": pull_count_map.values()})\n",
    "pull_open_count_df = pl.DataFrame(\n",
    "    {\"repo_id\": pull_open_count_map.keys(), \"n_open_pulls\": pull_open_count_map.values()}\n",
    ")\n",
    "\n",
    "print(\"PR情報を結合しています...\")\n",
    "\n",
    "train_merged = train_merged.join(pull_count_df, on='repo_id', how='left').fill_null(0)\n",
    "test_merged = test_merged.join(pull_count_df, on='repo_id', how='left').fill_null(0)\n",
    "\n",
    "train_merged = train_merged.join(pull_open_count_df, on='repo_id', how='left').fill_null(0)\n",
    "test_merged = test_merged.join(pull_open_count_df, on='repo_id', how='left').fill_null(0)\n",
    "\n",
    "print(train_merged[\"n_pulls\"])\n",
    "print(train_merged[\"n_open_pulls\"])\n",
    "\n",
    "train_merged = add_col(train_merged, \"pull_open_ratio\", train_merged[\"n_open_pulls\"] / (train_merged[\"n_pulls\"] + 0.01))\n",
    "test_merged = add_col(test_merged, \"pull_open_ratio\", test_merged[\"n_open_pulls\"] / (test_merged[\"n_pulls\"] + 0.01))\n",
    "\n",
    "print(train_merged[\"pull_open_ratio\"])\n",
    "print(\"PR情報の取り込みが完了しました\")"
   ],
   "id": "4ee5d199f7d3869c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 要素数を取得する関数\n",
    "def list_len(s: str):\n",
    "    return s.count(',')\n",
    "\n",
    "\n",
    "# 各データの \"n_stars\" に \"stars\" の要素数をいれる\n",
    "train_merged = add_col(train_merged, \"n_stars\", train_merged[\"stars\"].map_elements(list_len))\n",
    "test_merged = add_col(test_merged, \"n_stars\", test_merged[\"stars\"].map_elements(list_len))\n",
    "# train_merged[\"n_stars\"] = train_merged[\"stars\"].apply(list_len)\n",
    "# test_merged[\"n_stars\"] = test_merged[\"stars\"].apply(list_len)\n",
    "\n",
    "train_merged = add_col(train_merged, \"n_recent_stars\", train_merged[\"stars\"].map_elements(\n",
    "    lambda x: len(list(\n",
    "        filter(\n",
    "            # 2022/1/1 = 1640995200\n",
    "            lambda y: dt.fromisoformat(y[\"created_at\"]).timestamp() > 1640995200 - 31 * 24 * 60 * 60,\n",
    "            eval(x)\n",
    "        )\n",
    "    ))\n",
    "))\n",
    "test_merged = add_col(test_merged, \"n_recent_stars\", test_merged[\"stars\"].map_elements(list_len))\n",
    "\n",
    "# 各データの \"n_files\" に \"files\" の要素数をいれる\n",
    "train_merged = add_col(train_merged, \"n_files\", train_merged[\"files\"].map_elements(list_len))\n",
    "test_merged = add_col(test_merged, \"n_files\", test_merged[\"files\"].map_elements(list_len))\n",
    "# train_merged[\"n_files\"] = train_merged[\"files\"].apply(list_len)\n",
    "# test_merged[\"n_files\"] = test_merged[\"files\"].apply(list_len)\n",
    "\n",
    "# n_starsの分布を表示\n",
    "# plt.hist(train_merged['n_stars'], bins=50, alpha=0.5, label='train', log=True)\n",
    "# plt.hist(test_merged['n_stars'], bins=50, alpha=0.5, label='test', log=True)\n",
    "# plt.show()\n",
    "\n",
    "# n_filesの分布を表示\n",
    "# plt.hist(train_merged['n_files'], bins=50, alpha=0.5, label='train', log=True)\n",
    "# plt.hist(test_merged['n_files'], bins=50, alpha=0.5, label='test', log=True)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# \"star_file_ratio\" に n_files / n_stars を代入 (スターが多いほど小さく、ファイルが多いほど大きくなる)\n",
    "train_merged = add_col(train_merged, \"star_file_ratio\", train_merged[\"n_files\"] / train_merged[\"n_stars\"])\n",
    "test_merged = add_col(test_merged, \"star_file_ratio\", test_merged[\"n_files\"] / test_merged[\"n_stars\"])\n",
    "# train_merged[\"star_file_ratio\"] = train_merged[\"n_files\"] / train_merged[\"n_stars\"]\n",
    "# test_merged[\"star_file_ratio\"] = test_merged[\"n_files\"] / test_merged[\"n_stars\"]\n",
    "\n",
    "# \"file_par_commit\" に n_files / n_commits を代入 (commitあたりの平均ファイル数)\n",
    "train_merged = add_col(train_merged, \"file_par_commit\", train_merged[\"n_files\"] / train_merged[\"n_commits\"])\n",
    "test_merged = add_col(test_merged, \"file_par_commit\", test_merged[\"n_files\"] / test_merged[\"n_commits\"])\n",
    "# train_merged[\"file_par_commit\"] = train_merged[\"n_files\"] / train_merged[\"n_commits\"]\n",
    "# test_merged[\"file_par_commit\"] = test_merged[\"n_files\"] / test_merged[\"n_commits\"]\n",
    "\n",
    "train_merged = add_col(train_merged, \"star_par_commit\", train_merged[\"n_stars\"] / train_merged[\"n_commits\"])\n",
    "test_merged = add_col(test_merged, \"star_par_commit\", test_merged[\"n_stars\"] / test_merged[\"n_commits\"])\n",
    "\n",
    "import math\n",
    "\n",
    "# n_issues はlogを取ったほうが扱いやすい値なので操作\n",
    "# train_merged = add_col(train_merged, \"n_issues_log\", train_merged[\"n_issues\"].map_elements(math.log))\n",
    "# test_merged = add_col(test_merged, \"n_issues_log\", test_merged[\"n_issues\"].map_elements(math.log))\n",
    "# train_merged[\"n_issues_log\"] = train_merged[\"n_issues\"].apply(math.log)\n",
    "# test_merged[\"n_issues_log\"] = test_merged[\"n_issues\"].apply(math.log)\n"
   ],
   "id": "a3a729850f0c18f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def show_dist(df, key, log):\n",
    "    sns.displot(df[key], kde=False, rug=False, bins=500, log_scale=10 if log else None).set(title=f\"{key} log : all\")\n",
    "    sns.displot(df.filter(pl.col(\"active\") == True)[key], kde=False, rug=False, bins=500,\n",
    "                log_scale=10 if log else None).set(\n",
    "        title=f\"{key} log : true\")\n",
    "    sns.displot(df.filter(pl.col(\"active\") == False)[key], kde=False, rug=False, bins=500,\n",
    "                log_scale=10 if log else None).set(\n",
    "        title=f\"{key} log : false\")\n",
    "\n",
    "\n",
    "# fileの個数分布を表示\n",
    "# show_dist(train_merged, \"n_files\")\n",
    "# starの個数分布を表示\n",
    "# show_dist(train_merged, \"n_stars\")\n",
    "\n",
    "# show_dist(train_merged, \"star_file_ratio\")\n",
    "\n",
    "# show_dist(train_merged, \"n_commits\")\n",
    "\n",
    "# show_dist(train_merged, \"file_par_commit\")\n",
    "\n",
    "show_dist(train_merged, \"latest_closed_issue\", False)\n",
    "\n",
    "# KFoldでデータを分割\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=34)\n",
    "\n",
    "# 学習対象の行\n",
    "use_cols = [\"n_stars\", \"n_files\", \"star_file_ratio\", \"n_commits\", \"file_par_commit\", \"last_commit_date\",\n",
    "            \"n_commit_members\", \"n_issues\", \"n_pulls\", \"readme_size\", \"readme_size_cnt\", \"latest_closed_issue\",\n",
    "            \"file_size\", \"issue_open_ratio\", \"pull_open_ratio\", \"len_commit_messages\", \"star_par_commit\",\n",
    "            \"n_recent_commits\", \"issue_message_len\", \"ave_issue_body_len\"]\n",
    "target_col = \"active\"\n",
    "\n",
    "for train_index, valid_index in kf.split(train_merged):\n",
    "    train_data = train_merged[train_index]\n",
    "    valid_data = train_merged[valid_index]\n",
    "    print(\"学習データの数:\", len(train_data), \"検証データの数:\", len(valid_data))\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# 各分割をもらって、lightgbm のモデルを訓練して訓練した後のモデルを返す関数\n",
    "def train_fold(train_X: pd.DataFrame, train_y: pd.Series, valid_X: pd.DataFrame, valid_y: pd.Series,\n",
    "               trial) -> lgb.Booster:\n",
    "    # データセットを作成\n",
    "    lgb_train = lgb.Dataset(train_X, train_y)\n",
    "    lgb_valid = lgb.Dataset(valid_X, valid_y, reference=lgb_train)\n",
    "\n",
    "    # https://zenn.dev/robes/articles/d53ff6d665650f\n",
    "    # {'learning_rate': 0.036710796431638056, 'num_leaves': 74, 'min_data_in_leaf': 37, 'min_sum_hessian_in_leaf': 21, 'bagging_fraction': 0.1505235263266912, 'bagging_freq': 6, 'feature_fraction': 0.9944044553565908}\n",
    "    # {'learning_rate': 0.035829155667069366, 'num_leaves': 99, 'min_data_in_leaf': 27, 'min_sum_hessian_in_leaf': 47, 'bagging_fraction': 0.395652389722927, 'bagging_freq': 5, 'feature_fraction': 0.9581394359092842}\n",
    "    params = {\n",
    "        # 二値分類として解く\n",
    "        'objective': 'binary',\n",
    "        # 評価指標として auc と accuracy を使う\n",
    "        'metric': ['binary_logloss', 'binary_error'],\n",
    "        # 'learning_rate':\n",
    "        # # trial.suggest_uniform('learning_rate', 0.01, 0.05),\n",
    "        #     0.036,\n",
    "        # 'n_estimators': 1000,\n",
    "        # 'importance_type': 'gain',\n",
    "        # 'num_leaves':\n",
    "        # # trial.suggest_int('num_leaves', 10, 100),\n",
    "        #     90,\n",
    "        # 'min_data_in_leaf':\n",
    "        # # trial.suggest_int('min_data_in_leaf', 5, 50),\n",
    "        #     30,\n",
    "        # 'min_sum_hessian_in_leaf':\n",
    "        # # trial.suggest_int('min_sum_hessian_in_leaf', 5, 50),\n",
    "        #     35,\n",
    "        # 'lambda_l1': 0,\n",
    "        # 'lambda_l2': 0,\n",
    "        # 'bagging_fraction':\n",
    "        # # trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
    "        #     0.3,\n",
    "        # 'bagging_freq':\n",
    "        # # trial.suggest_int('bagging_freq', 0, 10),\n",
    "        #     5,\n",
    "        # 'feature_fraction':\n",
    "        # # trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
    "        #     0.98,\n",
    "        # 'random_seed': 42\n",
    "        # # --- --- ---\n",
    "        \"learning_rate\": 0.01,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'num_leaves': 31,\n",
    "        'random_state': 0,\n",
    "        'num_iterations': 500,\n",
    "    }\n",
    "\n",
    "    # 学習. auc が 100ステップ以上改善しないなら打ち切るように設定する\n",
    "    model = lgb.train(params, lgb_train, valid_sets=[lgb_valid])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def learn(trial):\n",
    "    # 各分割で学習した結果をいれた\n",
    "    models = []\n",
    "\n",
    "    for train_index, valid_index in kf.split(train_merged):\n",
    "        train_data = train_merged[train_index]\n",
    "        valid_data = train_merged[valid_index]\n",
    "\n",
    "        train_X = train_data[use_cols].to_pandas()\n",
    "        train_y = train_data[target_col].to_pandas()\n",
    "\n",
    "        valid_X = valid_data[use_cols].to_pandas()\n",
    "        valid_y = valid_data[target_col].to_pandas()\n",
    "\n",
    "        model = train_fold(train_X, train_y, valid_X, valid_y, trial)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    # `oof_pred` に今回訓練したモデルたちによる予測結果を格納する\n",
    "\n",
    "    oof_pred = np.zeros(len(train_merged))\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_merged)):\n",
    "        # バリデーションデータを取り出す\n",
    "        valid_data = train_merged[valid_index]\n",
    "        valid_X = valid_data[use_cols]\n",
    "\n",
    "        # 予測結果を出力\n",
    "        oof_pred[valid_index] = models[i].predict(valid_X)\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "    score = roc_auc_score(train_merged[\"active\"], oof_pred)\n",
    "\n",
    "    print(\"score: \", score)\n",
    "    # return score\n",
    "\n",
    "    # ROC 曲線のプロット\n",
    "    # fpr, tpr, thresholds = roc_curve(train_merged[\"active\"], oof_pred)\n",
    "    #\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {score}')\n",
    "    # plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # ランダムな分類器の基準線\n",
    "    # plt.xlabel(\"False Positive Rate\")\n",
    "    # plt.ylabel(\"True Positive Rate\")\n",
    "    # plt.title(\"ROC Curve\")\n",
    "    # plt.legend()\n",
    "    # plt.grid()\n",
    "    # plt.show()\n",
    "    return score, models, oof_pred\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    score, models, oof_pred = learn(trial)\n",
    "    return score\n",
    "\n",
    "\n",
    "# import optuna\n",
    "#\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=300)\n",
    "#\n",
    "# print('Number of finished trials:', len(study.trials))\n",
    "# print('Best trial:', study.best_trial.params)\n",
    "#\n",
    "# exit(0)\n",
    "\n",
    "score, models, oof_pred = learn(None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_importance(models: list):\n",
    "    feature_importance = pd.DataFrame()\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        _df = pd.DataFrame()\n",
    "        _df['feature_importance'] = model.feature_importance()\n",
    "        _df['column'] = model.feature_name()\n",
    "        _df['fold'] = i + 1\n",
    "        feature_importance = pd.concat([feature_importance, _df], axis=0, ignore_index=True)\n",
    "\n",
    "    order = feature_importance.groupby('column').sum()[['feature_importance']].sort_values('feature_importance',\n",
    "                                                                                           ascending=False).index\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxenplot(data=feature_importance, x='feature_importance', y='column', order=order)\n",
    "    plt.title('feature importance')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 各データをどれだけ重視したか見る\n",
    "plot_importance(models)\n",
    "\n",
    "# 予測値の分布を表示\n",
    "plt.hist(oof_pred, bins=20)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 提出\n",
    "\n",
    "sample_sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "\n",
    "pred = np.zeros(sample_sub.shape[0])\n",
    "\n",
    "for model in models:\n",
    "    pred += model.predict(test_merged[use_cols])\n",
    "\n",
    "pred /= len(models)\n",
    "\n",
    "sample_sub[\"pred\"] = pred\n",
    "\n",
    "plt.hist(pred, bins=30)\n",
    "plt.show()\n",
    "\n",
    "sample_sub.to_csv(\"submission.csv\", index=False)\n"
   ],
   "id": "169fdf838abeaa74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
